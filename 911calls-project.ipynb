{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emergency - 911 Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montgomery County, PA\n",
    "Data Science Bootcamp Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Background:\n",
    "\n",
    "   Montgomery County\n",
    " is the third-most populous county in the Commonwealth of Pennsylvania. locally referred to as Montco. Montgomery County is very diverse, ranging from farms and open land in Upper Hanover to densely populated rowhouse streets in Cheltenham.\n",
    "\n",
    "   911 Calls\n",
    " is an emergency telephone number created by Congress in 2004 as the 911 Implementation and Coordination Office (ICO), the National 911 Program is housed within the National Highway Traffic Safety Administration at the U.S. Department of Transportation and is a joint program with the National Telecommunication and Information Administration in the Department of Commerce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Description: \n",
    " This dataset contains emergency calls from Montgomery County, PA.\n",
    " It includes 663,522 calls records from 2015 to 2020 and 9 Features. \n",
    "    Link : https://www.kaggle.com/mchirico/montcoalert\n",
    "\n",
    "Feature Columns:\n",
    "    lat: String variable, Latitude\n",
    "    lng: String variable, Longitude\n",
    "    desc: String variable, Description of the Emergency Call\n",
    "    zip: String variable, ZIP Code\n",
    "    title: String variable, Title of Emergency\n",
    "    timeStamp: String variable, Date and time of the call, YYYY-MM-DD HH:MM:SS\n",
    "    twp: String variable, Township\n",
    "    addr: String variable, General Address\n",
    "    e: String variable, Dummy variable, Index column (always 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/BUDUR/Desktop/Data scinsce Bootcamp/project/archive/911.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Latitude, Longitude, Description, ZIP Code, Title of Emergency, Date and time, Township, General Address, Dummy variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Rows     :',df.shape[0])\n",
    "print('Columns  :',df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping column e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('e',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Missing values:',df.isnull().values.sum())\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['zip'].isnull().sum()/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['twp'].isnull().sum()/df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zip code contains 12% Nan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['twp'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Records with no townships are mostly dead ends. Lets skip them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zip = pd.DataFrame(df['zip'].value_counts().head(5))\n",
    "df_zip.rename(columns = {'zip':'Top 5'}, inplace = True)\n",
    "df_zip.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the top 5 zip codes for 911 calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 148 unique title of emergency codes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reason feature\n",
    "\n",
    "In the titles column there are \"Reasons/Departments\" specified before the title code. These are EMS, Fire, and Traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reason'] = df['title'].apply(lambda title: title.split(':')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title_code feature\n",
    "Using the same method from above, we are going to create a column with just the title code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_code'] = df['title'].apply(lambda title: title.split(':')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the most common Reason for a 911 call?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reason'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - The number one reason for 911 calls are Emergency Medical Services.\n",
    "  - Almost half of the reasons are for EMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(15, 5))\n",
    "\n",
    "sns.countplot(x='reason', data=df, order=df['reason'].value_counts().index, ax=axes[0])\n",
    "axes[0].set_title('Common Reasons for 911 Calls', size=15)\n",
    "axes[0].set(xlabel='Reason', ylabel='Count')\n",
    "\n",
    "\n",
    "df['reason'].value_counts().plot.pie(ax=axes[1])\n",
    "\n",
    "\n",
    "\n",
    "sns.despine(bottom=False, left=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The barcahrt shows the top 10 emergency calls from all the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(10, 5))\n",
    "sns.countplot(y='title', data=df, order=df['title'].value_counts().index)\n",
    "sns.despine(bottom=False, left=True)\n",
    "axes.set_ylim([9, 0])\n",
    "axes.set_title('Overall 911 Emregency Calls', size=15)\n",
    "axes.set(xlabel='Number of 911 Calls', ylabel='')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - Vehicle accidents are the number one reason people call 911.\n",
    "   - Disabled vehicle and fire alarm are in second and third place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic 911 Emergency Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The most common emergency titles are vehicle accident, disable vehicle and road obstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['reason']=='Traffic'].groupby('title_code').count()['lat'].sort_values(ascending=True).plot(kind='barh', figsize=(10, 5))\n",
    "plt.xlabel('Number of 911 Calls')\n",
    "plt.ylabel('')\n",
    "plt.title('Traffic 911 Emergency Calls', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fire 911 Emergency Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The most common emergency titles are fire alarm, vehicle accident and fire investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['reason']=='Fire'].groupby('title_code').count()['lat'].sort_values(ascending=True).tail(10).plot(kind='barh', figsize=(10, 5))\n",
    "plt.xlabel('Number of 911 Calls')\n",
    "plt.ylabel('')\n",
    "plt.title('Fire 911 Emergency Calls', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMS 911 Emergency Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The most common emergency titles are fall victim, respiratory emergency and cardiac emergency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['reason']=='EMS'].groupby('title_code').count()['lat'].sort_values(ascending=True).tail(10).plot(kind='barh', figsize=(10, 5))\n",
    "plt.xlabel('Number of 911 Calls')\n",
    "plt.ylabel('')\n",
    "plt.title('EMS 911 Emergency Calls', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the timeStamp column from string to DateTime object to create 3 new columns called Hour, Month, and Day of Week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timeStamp'] = pd.to_datetime(df['timeStamp'])\n",
    "\n",
    "df['Hour'] = df['timeStamp'].apply(lambda time: time.hour)\n",
    "df['Month'] = df['timeStamp'].apply(lambda time: time.month)\n",
    "df['Day of Week'] = df['timeStamp'].apply(lambda time: time.dayofweek)\n",
    "dmap = {0:'Mon',1:'Tue',2:'Wed',3:'Thu',4:'Fri',5:'Sat',6:'Sun'}\n",
    "\n",
    "df['Day of Week'] = df['Day of Week'].map(dmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekly and monthly calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It looks like friday is the day with more calls during the week.\n",
    "- Regarding the monthly calls, looks like during the first semester there are more calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(15,5))\n",
    "\n",
    "sns.countplot(x='Day of Week', data=df, palette='viridis', ax=axes[0])\n",
    "axes[0].set_title('Weekly Calls', size=15)\n",
    "\n",
    "sns.countplot(x='Month', data=df, hue='reason', palette='viridis', ax=axes[1])\n",
    "axes[1].set_title('Monthly Calls', size=15)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "\n",
    "sns.despine(bottom=False, left=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date feature\n",
    "\n",
    "Create a new column called 'Date' that contains the date from the timeStamp column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = df['timeStamp'].apply(lambda t: t.date())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now groupby this Date column with the count() aggregate and create a plot of counts of 911 calls by reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['reason']=='Traffic'].groupby('Date').count()['lat'].plot(figsize=(15,5), color='darkblue')\n",
    "plt.title('Traffic', fontsize=15)\n",
    "sns.despine(bottom=False, left=True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['reason']=='Fire'].groupby('Date').count()['lat'].plot(figsize=(15,5), color='darkred')\n",
    "plt.title('Fire', fontsize=15)\n",
    "sns.despine(bottom=False, left=True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['reason']=='EMS'].groupby('Date').count()['lat'].plot(figsize=(15,5), color='darkgreen')\n",
    "plt.title('EMS', fontsize=15)\n",
    "sns.despine(bottom=False, left=True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the heatmap we can see that during 14:00 and 17:00 hours there are more calls.\n",
    "- Friday and Wednesday have more calls.\n",
    "- Apparently during Sunday the calls drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dayHour = df.groupby(by=['Day of Week', 'Hour']).count()['reason'].unstack()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(dayHour, linewidths=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling - 911 Call Type Prediction\n",
    "\n",
    "## Can we predict the type reason of the next call?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16428/170519341.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/BUDUR/Desktop/Data scinsce Bootcamp/project/archive/911.csv', nrows=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequences(texts, vocab_length=10000):\n",
    "    tokenizer = Tokenizer(num_words=vocab_length)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    \n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    \n",
    "    max_seq_length = np.max([len(sequence) for sequence in sequences])\n",
    "    \n",
    "    sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode(df, columns, prefixes):\n",
    "    df = df.copy()\n",
    "    \n",
    "    for column, prefix in zip(columns, prefixes):\n",
    "        dummies = pd.get_dummies(df[column], prefix=prefix)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "        df = df.drop(column, axis=1)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_inputs(df):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Create label column and drop the title column\n",
    "    df['type'] = df['title'].apply(lambda x: re.search(r'^\\w+', x).group(0))\n",
    "    df = df.drop('title', axis=1)\n",
    "  \n",
    "    \n",
    "    # Get sequences for desc and addr columns (and drop original columns)\n",
    "    vocab_length = 10000\n",
    "    desc_sequences = get_sequences(df['desc'], vocab_length=vocab_length)\n",
    "    addr_sequences = get_sequences(df['addr'], vocab_length=vocab_length)\n",
    "    df = df.drop(['desc', 'addr'], axis=1)\n",
    "    \n",
    "    # One-hot encode remaining categorical columns (zip and twp)\n",
    "    df = onehot_encode(df, columns=['zip', 'twp'], prefixes=['z', 't'])\n",
    "    \n",
    "    # Split df into X and y \n",
    "    y = df['type'].copy()\n",
    "    X = df.drop('type', axis=1).copy()\n",
    "    \n",
    "    # Map labels to integers\n",
    "    label_mapping = {'EMS': 0, 'Traffic': 1, 'Fire': 2}\n",
    "    y = y.replace(label_mapping)\n",
    "    \n",
    "    # Scale X with a standard scaler\n",
    "    scaler = StandardScaler()\n",
    "    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    return X, desc_sequences, addr_sequences, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, desc_sequences, addr_sequences, y = preprocess_inputs(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addr_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, desc_train, desc_test, addr_train, addr_test, y_train, y_test = \\\n",
    "    train_test_split(X, desc_sequences, addr_sequences, y, train_size=0.7, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_inputs = tf.keras.Input(shape=(X_train.shape[1],))\n",
    "desc_inputs = tf.keras.Input(shape=(desc_train.shape[1],))\n",
    "addr_inputs = tf.keras.Input(shape=(addr_train.shape[1],))\n",
    "\n",
    "# X_inputs\n",
    "X_dense1 = tf.keras.layers.Dense(128, activation='relu')(X_inputs)\n",
    "X_dense2 = tf.keras.layers.Dense(128, activation='relu')(X_dense1)\n",
    "\n",
    "# desc_inputs\n",
    "desc_embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=10000,\n",
    "    output_dim=64,\n",
    "    input_length=desc_train.shape[1]\n",
    ")(desc_inputs)\n",
    "desc_flatten = tf.keras.layers.Flatten()(desc_embedding)\n",
    "\n",
    "# addr_inputs\n",
    "addr_embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=10000,\n",
    "    output_dim=64,\n",
    "    input_length=addr_train.shape[1]\n",
    ")(addr_inputs)\n",
    "addr_flatten = tf.keras.layers.Flatten()(addr_embedding)\n",
    "\n",
    "# Concatenate results\n",
    "concat = tf.keras.layers.concatenate([X_dense2, desc_flatten, addr_flatten])\n",
    "\n",
    "# Make predictions\n",
    "outputs = tf.keras.layers.Dense(3, activation='softmax')(concat)\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs=[X_inputs, desc_inputs, addr_inputs], outputs=outputs)\n",
    "\n",
    "print(model.summary())\n",
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train, desc_train, addr_train],\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ReduceLROnPlateau()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate([X_test, desc_test, addr_test], y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model loss: {:.5f}\".format(results[0]))\n",
    "print(\"Model accuracy: {:.2f}%\".format(results[1] * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
